{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "24946\n"
    }
   ],
   "source": [
    "# CONCOLUTIONAR NN\n",
    "# The difference between NN and CNN is that uses the convolution to locate features of an image. And it\n",
    "# uses multiple instances of this convolution that enable it to condense for example an image.\n",
    "# these convolutions are done by making a smaller square on the image, moving it around and saving each piece.\n",
    "# Then the machine takes each small features and uses them to create a new condensed version of this image\n",
    "# using the pieces (features). Then follows a process of polling where the maximum value inside each window\n",
    "# is chosen and then replaces the part of image with this piece.\n",
    "# So in summary CNN is drastically simplifying the image and looking for features in it.\n",
    "# Simple: Reduces your image to simple building blocks and then finds patterns of these blocks given how many\n",
    "# layers you have.\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "REBUILD_DATA = False\n",
    "\n",
    "# We generally don't need a class for this, but in our case (image processing) there are quite a few steps\n",
    "# we are/have to make like the same methods and functions...\n",
    "# Important note to make here is that for iamge classification many trick can be used to make our dataset\n",
    "# more rounded and higher the accuarcy of generalization. These tricks involve: croping images, resizing\n",
    "# them and adding white spaces or even rotating them and using the reotated versions as new images which\n",
    "# can increase the ammount of our data by 4x (4 ways of rotating an image) or more...\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50 # 50x50 pixels\n",
    "    CATS = \"Kaggle/PetImages/Cat\"\n",
    "    DOGS = \"Kaggle/PetImages/Dog\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "    # A important fact to mention is the importance of balanced ammount of data for each class we are trying\n",
    "    # to disscerne. Therefore we will create counters here for each class\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "    \n",
    "    def make_training_data(self):\n",
    "        # iterate trough our dictionary of classes\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            # iterate trough images in directory\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try: \n",
    "                    # We use the os.path.join functrion to add a label (0 or 1) to each image\n",
    "                    path = os.path.join(label, f)\n",
    "                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) # converting to GRAYSCALE is not a necessity\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # For some images there is an exception, maybe it's because they are corrupted or \n",
    "                    # maybe it's the resize...\n",
    "                    pass\n",
    "                    #print(str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print(\"Cats:\", self.catcount)\n",
    "        print(\"Dogs:\", self.dogcount)\n",
    "\n",
    "# if we want to rebuild everything (takes long time)\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "    \n",
    "# We extract the training data so we don't have to create it again, for speeds sake\n",
    "# There is an issue with the pickle function.\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we build the convolutional network for this we have to import PyTorch-es libraries. We need the general torch library for tensors, the NN module for convolutional functionality and we specifically save the functional part of the NN module in to F so we can access it through there.\n",
    "\n",
    "We start by defining a `Net` function, which will work as a constructor for our NN. We defien a initialization class `__init__(self)` which inherits it's parent init class form the `nn.Module`.\n",
    "Let's start the init by defining 3 convolutional layers (`conv1, conv2, conv3`), for which we use `nn.Conv2d` - \n",
    "We define the 2d convolutional function with 3 parameters `in_channels, out_channels, kernel_size`, where:\n",
    "- `in_channels`: number of channels in the input image\n",
    "- `out_channels`: number of channels produced by the convolution\n",
    "- `kernel_size`: size of the convolving kernel - int(5) = 5x5; tuple(5, 3) = 5x3 kernel\n",
    "\n",
    "Next we use a max pooling function, which is a *sample-based discretizaion process*, whose objective is to down-sample an input (in our case an image), which reduces its dimensionality by making assumptions about the features contained in the sub-regions of the sample being pooled.\n",
    "\n",
    "** Max pooling example: **\n",
    "\n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I our case we use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Finished running\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        self.pool3 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512) \n",
    "        self.fc2 = nn.Linear(512, 2) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = x.flatten(start_dim=1) # flattening out\n",
    "        #print(x.shape) # We print the shape for fc1 Linear\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "#net.forward(torch.randn(1, 1, 50, 50)) # passing random sample data, to determine size of the fc1 layer input\n",
    "print(\"Finished running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2494\n"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "22452\n2494\n"
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 225/225 [01:26<00:00,  2.59it/s]\ntensor(0.2171, grad_fn=<MseLossBackward>)\n"
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "       #print(i, i+BATCH_SIZE)\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 2494/2494 [00:13<00:00, 182.10it/s]\nAccuarcy:0.626\n"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "print(\"Accuarcy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}